{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing and storage\n",
    "The following sections will handle the scraped data stored in json files. We start by opening our data file, in our case it's raw_study_in_germany_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the crawler's data from the output file\n",
    "\n",
    "import json\n",
    "file_path = '../temporary_folder/handbook-germany.json'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    raw_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create functions to handle and remove personal information from the raw scraped data. We also make an LLM call (OpanAI API) for further processing and to define whether the resulting information is relevant to students who would like to study/live in Germany.\n",
    "\n",
    "Before running this code, make sure to run `python -m spacy download en_core_web_sm` in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import spacy\n",
    "\n",
    "# Optional: Import OpenAI if using LLM-based cleaning\n",
    "try:\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')  # Ensure your API key is set as an environment variable\n",
    "    client = OpenAI()\n",
    "except ImportError:\n",
    "    OpenAI = None\n",
    "    print(\"OpenAI library not found. LLM-based cleaning will be skipped.\")\n",
    "\n",
    "# Initialize spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def remove_emails(text):\n",
    "    \"\"\"Remove email addresses from text.\"\"\"\n",
    "    email_pattern = r'\\S+@\\S+'\n",
    "    return re.sub(email_pattern, '[EMAIL]', text)\n",
    "\n",
    "def remove_phone_numbers(text):\n",
    "    \"\"\"Remove phone numbers from text.\"\"\"\n",
    "    phone_pattern = r'\\+?\\d[\\d -]{8,}\\d'\n",
    "    return re.sub(phone_pattern, '[PHONE]', text)\n",
    "\n",
    "def remove_names(text):\n",
    "    \"\"\"Remove person names using spaCy's NER.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "    cleaned_text = text\n",
    "    # Replace names with [NAME]\n",
    "    for start, end, label in reversed(entities):\n",
    "        cleaned_text = cleaned_text[:start] + '[NAME]' + cleaned_text[end:]\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_addresses(text):\n",
    "    \"\"\"Remove addresses using regex and spaCy.\"\"\"\n",
    "    # Simple regex pattern for addresses (can be enhanced)\n",
    "    address_pattern = r'\\d{1,5}\\s\\w+\\s(?:Street|St|Avenue|Ave|Boulevard|Blvd|Road|Rd|Lane|Ln|Drive|Dr)\\b[\\w\\s,.-]*'\n",
    "    text = re.sub(address_pattern, '[ADDRESS]', text)\n",
    "    \n",
    "    # Using spaCy to remove GPE (Geopolitical Entities) as a proxy for locations\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents if ent.label_ in ['GPE', 'LOC']]\n",
    "    cleaned_text = text\n",
    "    # Replace locations with [LOCATION]\n",
    "    for start, end, label in reversed(entities):\n",
    "        cleaned_text = cleaned_text[:start] + '[LOCATION]' + cleaned_text[end:]\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_personal_info(text):\n",
    "    \"\"\"Aggregate function to remove various personal information.\"\"\"\n",
    "    text = remove_emails(text)\n",
    "    text = remove_phone_numbers(text)\n",
    "    text = remove_names(text)\n",
    "    text = remove_addresses(text)\n",
    "    return text\n",
    "\n",
    "def define_relevance_with_llm(text):\n",
    "    relevance_prompt=f\"\"\"\n",
    "    You are an automated classifier designed to evaluate whether a given text \n",
    "    contains relevant information for students interested in studying and living\n",
    "    in Germany. Your task is to read the provided text and determine its relevancy\n",
    "    based on the following criteria:\n",
    "\n",
    "    Respond with \"Yes\" if the text includes information such as:\n",
    "\n",
    "    Study opportunities (e.g., universities, courses, scholarships)\n",
    "    Living conditions (e.g., cost of living, accommodation tips)\n",
    "    Cultural insights (e.g., cultural norms, language tips)\n",
    "    Practical advice for adapting to life in Germany\n",
    "    Local resources and support systems for international students\n",
    "    Respond with \"No\" if the text:\n",
    "\n",
    "    Does not pertain to studying or living in Germany\n",
    "    Contains irrelevant or unrelated information\n",
    "    Focuses on personal anecdotes without broader applicability\n",
    "    Discusses topics outside the scope of student life in Germany\n",
    "    Examples:\n",
    "\n",
    "    Text: \"Discover the top universities in Germany offering scholarships\n",
    "    for international students. Learn about the application process and tips\n",
    "    for living on a student budget.\"\n",
    "\n",
    "    Response: Yes\n",
    "\n",
    "    Text: \"The weather today is sunny with a high of 25°C. It's a perfect day\n",
    "    for a picnic in the park.\"\n",
    "\n",
    "    Response: No\n",
    "\n",
    "    Text: \"Navigating the German public transportation system can be challenging\n",
    "    for newcomers. Here's a guide to help you get around efficiently.\"\n",
    "\n",
    "    Response: Yes\n",
    "\n",
    "    Text: \"I went hiking in the Alps last summer and it was an unforgettable experience.\"\n",
    "\n",
    "    Response: No\n",
    "\n",
    "    Now, evaluate the following text:\n",
    "\n",
    "    Text: \"{text}\"\n",
    "\n",
    "    Response:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": relevance_prompt},\n",
    "        ])\n",
    "        \n",
    "        relevance = response.choices[0].message.content\n",
    "        return relevance\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LLM-based relevance detection: {e}\")\n",
    "        return text\n",
    "    \n",
    "\n",
    "def clean_text_with_llm(text):\n",
    "    \"\"\"\n",
    "    Clean text using an LLM like OpenAI's GPT.\n",
    "    This function sends a prompt to the LLM to clean the text.\n",
    "    Ensure you have set the OPENAI_API_KEY environment variable.\n",
    "    \"\"\"\n",
    "    if not client:\n",
    "        print(\"OpenAI library not available. Skipping LLM-based cleaning.\")\n",
    "        return text\n",
    "    \n",
    "    cleaning_prompt = f\"\"\"\n",
    "    You are an expert data cleaner tasked with processing scraped website data \n",
    "    to create clear and informative content for students interested in studying\n",
    "    and living in Germany. The input text may contain disorganized information and \n",
    "    personal details. Your objectives are to:\n",
    "\n",
    "    Extract Relevant Information:\n",
    "\n",
    "    Identify and retain only the information pertinent to studying and living in \n",
    "    Germany, such as educational opportunities, living conditions, cultural insights,\n",
    "    accommodation tips, and local resources.\n",
    "    \n",
    "    Remove Personal Information:\n",
    "\n",
    "    Exclude all personal data, including names, contact details, addresses, email addresses,\n",
    "    phone numbers, and any other identifying information.\n",
    "    Ensure Coherence and Clarity:\n",
    "\n",
    "    Organize the extracted information into a well-structured, coherent, and readable format.\n",
    "    Maintain the original meaning and intent of the content without adding or omitting critical information.\n",
    "    \n",
    "    Original Text: \"{text}\"\n",
    "\n",
    "    Cleaned Text:\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": cleaning_prompt},\n",
    "        ])\n",
    "        \n",
    "        cleaned_text = response.choices[0].message.content\n",
    "        return cleaned_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LLM-based cleaning: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use all our previous functions to clean the data. Then store it in a separate json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(INPUT_FILE = '../temporary_folder/handbook-germany.json',\n",
    "    CLEANED_FILE = '../temporary_folder/handbook-germany-cleaned.json'):\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Input file '{INPUT_FILE}' not found.\")\n",
    "        return\n",
    "\n",
    "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            print(f\"Loaded {len(data)} data points from '{INPUT_FILE}'.\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print(\"\\nInitial DataFrame Info:\")\n",
    "    print(df.info())\n",
    "\n",
    "    required_columns = ['url', 'title', 'text_content']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Missing required column: '{col}'. Please check your data.\")\n",
    "            return\n",
    "\n",
    "    initial_count = len(df)\n",
    "    df.drop_duplicates(subset=['url'], inplace=True)\n",
    "    duplicates_removed = initial_count - len(df)\n",
    "    print(f\"\\nRemoved {duplicates_removed} duplicate entries based on 'url'.\")\n",
    "\n",
    "    initial_count = len(df)\n",
    "    df.dropna(subset=['url', 'text_content'], inplace=True)\n",
    "    missing_removed = initial_count - len(df)\n",
    "    print(f\"Removed {missing_removed} entries with missing 'url' or 'text_content'.\")\n",
    "\n",
    "    titles_filled = df['title'].isnull().sum()\n",
    "    df['title'].fillna('No Title', inplace=True)\n",
    "    print(f\"Filled {titles_filled} missing 'title' entries with 'No Title'.\")\n",
    "\n",
    "    print(\"\\nRemoving personal information...\")\n",
    "    df['text_content'] = df['text_content'].apply(remove_personal_info)\n",
    "\n",
    "    apply_llm = True \n",
    "    if apply_llm:\n",
    "        print(\"Applying LLM-based cleaning...\")\n",
    "        df['text_content'] = df['text_content'].apply(clean_text_with_llm)\n",
    "\n",
    "    df.to_json(CLEANED_FILE, orient='records', indent=4)\n",
    "    print(f\"\\nCleaned data saved to '{CLEANED_FILE}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 175 data points from '../temporary_folder/handbook-germany.json'.\n",
      "\n",
      "Initial DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175 entries, 0 to 174\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   url           175 non-null    object\n",
      " 1   title         175 non-null    object\n",
      " 2   text_content  175 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 4.2+ KB\n",
      "None\n",
      "\n",
      "Removed 0 duplicate entries based on 'url'.\n",
      "Removed 0 entries with missing 'url' or 'text_content'.\n",
      "Filled 0 missing 'title' entries with 'No Title'.\n",
      "\n",
      "Removing personal information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35569/590502822.py:37: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['title'].fillna('No Title', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LLM-based cleaning...\n",
      "\n",
      "Cleaned data saved to '../temporary_folder/handbook-germany-cleaned.json'.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have a cleaned json file, we can now convert it to a txt file for storage in a vector database (Alternatively, we can pause here and return later since our data is present locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../temporary_folder/handbook-germany-cleaned.json', 'r', encoding='utf-8') as file:\n",
    "    clean_data = json.load(file)\n",
    "    \n",
    "output_txt_path = \"../temporary_folder/handbook-germany-cleaned.txt\"\n",
    "\n",
    "if isinstance(clean_data, list):\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as txt_file:\n",
    "        for entry in clean_data:\n",
    "            if isinstance(entry, dict) and 'text_content' in entry:\n",
    "                text = entry['text_content']\n",
    "                txt_file.write(text + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now store the data in a vector database. We are using QDRANT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rissalhedna/Desktop/unillm/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00,  4.74it/s]\n",
      "Generating embeddings: 100%|██████████| 384/384 [00:06<00:00, 60.85it/s]\n"
     ]
    }
   ],
   "source": [
    "import rootutils\n",
    "notebook_path = \"../notebooks/scraping_cleaning_pipeline.ipynb\"\n",
    "\n",
    "rootutils.setup_root(search_from=notebook_path, indicator='.project_root', pythonpath=True)\n",
    "\n",
    "from app.utils.storage_utils import store_in_qdrant, query_qdrant\n",
    "import qdrant_client\n",
    "import os\n",
    "\n",
    "# Get environment variables for Qdrant configuration\n",
    "QDRANT_URL = os.environ['QDRANT_URL']\n",
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY']\n",
    "ENVIRONMENT = os.environ['ENVIRONMENT']\n",
    "\n",
    "# Initialize Qdrant client based on environment\n",
    "if ENVIRONMENT == 'dev':\n",
    "    # For local development, connect to localhost\n",
    "    client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333,)\n",
    "else:\n",
    "    # For production, connect to cloud instance using URL and API key\n",
    "    client = qdrant_client.QdrantClient(\n",
    "        QDRANT_URL,\n",
    "        api_key=QDRANT_API_KEY\n",
    "    )\n",
    "\n",
    "store_in_qdrant(client, \"study-in-germany\", file_path=\"../temporary_folder/handbook-germany-cleaned.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now query the vector database to test if it's working. The LLM will return the most relevant information based on the query. Any information that is not relevant to the query will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"I don't know.\",\n",
       " 'sources': [{'file_name': 'handbook-germany-cleaned.txt'},\n",
       "  {'file_name': 'handbook-germany-cleaned.txt'},\n",
       "  {'file_name': 'handbook-germany-cleaned.txt'},\n",
       "  {'file_name': 'handbook-germany-cleaned.txt'},\n",
       "  {'file_name': 'handbook-germany-cleaned.txt'},\n",
       "  {'file_name': 'handbook-germany-cleaned.txt'},\n",
       "  {'file_name': 'handbook-germany-cleaned.txt'},\n",
       "  {'file_name': 'handbook-germany-cleaned.txt'},\n",
       "  {'file_name': 'handbook-germany-cleaned.txt'},\n",
       "  {'file_name': 'handbook-germany-cleaned.txt'}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_qdrant(client, \"study-in-germany\", \"On average how much do student working jobs pay in computer science ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'To find an apartment in Germany, you can use several online platforms that are popular for rental listings. Here are some recommended options:\\n\\n1. **eBay Kleinanzeigen**: A widely used classifieds website where you can find various rental listings, including private apartments and shared flats.\\n\\n2. **Immobilienscout24**: One of the largest real estate platforms in Germany, offering a comprehensive range of rental properties, including apartments and houses.\\n\\n3. **Immonet**: Another popular real estate website that provides listings for rentals across Germany.\\n\\n4. **Wunderflats**: This platform specializes in furnished apartments for temporary stays, making it a good option if you are looking for short-term accommodation.\\n\\n5. **Nena Apartments**: A platform that focuses on furnished apartments, suitable for both short and long-term rentals.\\n\\n6. **Host4ukraine.com** and **icanhelp.host**: These websites are specifically designed to help those in need of accommodation, particularly for individuals from Ukraine.\\n\\n7. **Facebook Groups**: Local Facebook groups can also be valuable resources for finding available flats or shared accommodations, as well as connecting with other students or residents.\\n\\n8. **University Bulletin Boards**: Many universities have bulletin boards (both physical and online) where students can post or find rental listings.\\n\\nAdditionally, consider using the App Point for searching temporary accommodation and visiting uahelp.wiki/accommodation for more platforms and resources. Always ensure to verify listings and be cautious of potential scams.',\n",
       " 'sources': [{'file_name': 'handbook-germany-cleaned.txt'},\n",
       "  {'file_name': 'handbook-germany-cleaned.txt'},\n",
       "  {'file_name': 'handbook-germany-cleaned.txt'},\n",
       "  {'file_name': 'handbook-germany-cleaned.txt'},\n",
       "  {'file_name': 'handbook-germany-cleaned.txt'},\n",
       "  {'file_name': 'handbook-germany-cleaned.txt'},\n",
       "  {'file_name': 'handbook-germany-cleaned.txt'},\n",
       "  {'file_name': 'handbook-germany-cleaned.txt'},\n",
       "  {'file_name': 'handbook-germany-cleaned.txt'},\n",
       "  {'file_name': 'handbook-germany-cleaned.txt'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_qdrant(client, \"study-in-germany\", \"What rental platform should i use to find an apartment?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_qdrant(client, \"study-in-germany\", \"How can I \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
